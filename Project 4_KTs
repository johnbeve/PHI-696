
**The SPARQL Library of Buffalo**

[Codewars](https://www.codewars.com/dashboard) is a website designed to facilitate algorithmic training for various programming languages. Users supply problem statements and others provide coding solutions to those problems. For example, you might find a problem for Python such as: 

Define a function that returns the length of a given string. 

With a solution like: 

def length_of_string(s):
	return len(s)


Codewars is not limited to traditional programming languages like Python, but also facilitates training for languages like SQL. As you have learned, SQL and SPARQL are both query languages, but what might surprise you is that there is currently no option for training SPARQL in Codewars. This project will go some way to remedy that. 

For this project, you will be tasked with constructing SPARQL problems for the codewars site. 

```
Note #1: Completion of this task will not require you to actually have your SPARQL problems successfully posted to codewars. Adding problems to codewars takes more time than we have for this project. Additionally, you are only allowed to add propose problems to codewars if you have a certain amount of experience (specifically, you need 300 of what they call 'honor points', which is acquired by solving problems). At some point, assuming you permit it, I will post your problems to codewars (giving you credit of course). 

Note #2: The potential for this project to directly impact the ontology community is clear. SPARQL can be challenging, and there are few opportunities for drill practice like this. 

Note #3: You will not be required to learn a programming language, though you will likely need to expand your comfort with computer science jargon; if you hit a wall, ask your peers for help; if the wall persists, ask me. 

Note #4: Codewars provides a guidebook - https://docs.codewars.com/authoring/tutorials/create-first-kata/ - for creating problems; I strongly encourage you to read it, since the standard provided there is how I will be evaluating success. 
```
**Assignment Details**

Problems on Codewars are ranked in terms of difficulty. The lowest "kata" - 8 - indicates a rather easy problem, while the highest kata - 1 - indicates a very challenging problem. 

For our purposes, harder kata will be worth more points than easier kata, and you are required to submit enough kata to acquire 100 points according to the following point system: 

  |   **kata**    |  **points**   |
  | ------------- | ------------- |
  |       1       |      35       |
  |       2       |      25       |
  |       3       |      20       |
  |       4       |      10       |
  |       5       |       5       |
  |       6       |       3       |
  |       7       |       2       |
  |       8       |       0       |

You're probably thinking, "why would I submit a level 8 kata if they're not worth any points?" Great question. Because everyone had to submit at least one level 8 kata. Otherwise, you're permitted to submit kata in any distribution you choose. For example, you might submit 2 problems for kata one (70 points), one for kata 3 (20 points), one for kata 4 (10 points), and one for kata 8 (0 points but required). 

It is your responsibility and the responsibility of your peers reviewing your submission in PR to determine whether your submission is ranked appropriately. In the event that consensus is reached that your kata is ranked inappropriately, you must work with your peers to revise the submission so that it is either more or less challenging, accordingly. You are not permitted to submit new problems with different strengths after PRs are open, but must instead revise your PRs. So, think hard about how challenging your submission is. 

There is one other option for those desiring a different sort of challenge. If you provide alongside your SPARQL submission a translation of the same problem into SQL, complete with documentations, solution, etc. then you may receive half points extra at that kata level (rounded up). For example, if you submit a SPARQL problem that is kata rank 1 and also submit a SQL version of that same problem, you  will receive 35+18=53 points.

---





(1) Kata level-8 (0/100 points; required.)

Title: Retrieve the name of a city  

Description:

You are new to SPARQL, welcome! To get you started, this kata will ask that you retrieve a specific piece of data by writing a SPARQL query. Write a SPARQL query that retrieves the name of a city. You will be given the URI of a city, and your task is to extract its name using the DBpedia ontology.

Example:

Suppose you are given the URI of Paris: http://dbpedia.org/resource/Paris. Your task is to write a SPARQL query that retrieves the name of this city.


Answer:

---
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?cityLabel
WHERE {
  <http://dbpedia.org/resource/Paris> rdfs:label ?cityLabel .
  FILTER (lang(?cityLabel) = 'en')
}
---

Return on endpoint:

cityLabel
-------------
"Paris"@en


Explanation:

The first line defines the prefix for the RDF Schema namespace.
The SELECT statement retrieves the label of the given city using the WHERE clause.
The WHERE clause specifies the city URI as the subject, and rdfs:label as the predicate.
The FILTER clause restricts the results to the English language labels.







(2) Kata level-6 (3/100 points; bal. 97)

Title: Retrieve the titles of books authored by Jorge Luis Borges

Description:

Your roommate just read The Aleph by Borges and doesn't shut up about it. They expressed interest in reading more works by this author, but lament tha they are too busy this week to research more titles. You want to surprise your roomie with a list of titles by their new favorite author, but instead of researching and finding the titles one by one you figure you should use your newly-gained knowledge of SPARQL. Write a SPARQL query that retrieves the titles of books authored by Jorge Luis Borges. You will be given the URI of Borges, and your task is to extract the titles of the books he authored using the DBpedia ontology.

Example:

Suppose you are given the URI of Borges: http://dbpedia.org/resource/Jorge_Luis_Borges. Your task is to write a SPARQL query that retrieves the titles of books he authored.

Solution to SPARQL query:

---
PREFIX dbo: <http://dbpedia.org/ontology/>

SELECT ?bookTitle
WHERE {
  <http://dbpedia.org/resource/Jorge_Luis_Borges> dbo:notableWork ?book .
  ?book dbo:author <http://dbpedia.org/resource/Jorge_Luis_Borges> .
  ?book dbo:name ?bookTitle .
}
ORDER BY ?bookTitle
---

Explanation:

The first line defines the prefix for the DBpedia ontology namespace.
The SELECT statement retrieves the title of the books authored by Borges using the WHERE clause.
The WHERE clause specifies that the book must have Borges as the author using the dbo:author property, and must be a notable work using the dbo:notableWork property.
The ?book variable refers to the book URI, and ?bookTitle refers to the book title.
The results are sorted in ascending order by book title using the ORDER BY clause.


Answer:

Here are some results from this query:

bookTitle
------------------------------------------
A Universal History of Iniquity
Atlas
Dreamtigers
El aleph
El hacedor
Ficciones
Historia universal de la infamia
Inquisiciones
La cuestión del otro
La muerte y la brújula
La biblioteca de Babel
La lotería en Babilonia y otros relatos
Nueve ensayos dantescos
Otras inquisiciones
Pierre Menard, autor del Quijote
The Book of Imaginary Beings
The Garden of Forking Paths
The Maker
The Secret Miracle
The South
The Theme of the Traitor and the Hero

















(3) Kata level-5 (5/100 points; bal. 92)

Title: Retrieve the titles, importance, and popularity of books authored by Jorge Luis Borges

Description:

You are not satisfied with simply listing out book titles and want to take your query to the next level. Write a SPARQL query that retrieves the titles, importance, and popularity of books authored by Jorge Luis Borges. You will be given the URI of Borges, and your task is to extract the titles of the books he authored, along with information on why they are important and why they are popular, using the DBpedia ontology.

Example:

Suppose you are given the URI of Borges: http://dbpedia.org/resource/Jorge_Luis_Borges. Your task is to write a SPARQL query that retrieves the titles, importance, and popularity of books he authored.


Solution to SPARQL query:

---
PREFIX dbo: <http://dbpedia.org/ontology/>
PREFIX dbp: <http://dbpedia.org/property/>

SELECT DISTINCT ?bookTitle ?importance ?popularity
WHERE {
  <http://dbpedia.org/resource/Jorge_Luis_Borges> dbo:notableWork ?book .
  ?book dbo:author <http://dbpedia.org/resource/Jorge_Luis_Borges> .
  ?book dbo:name ?bookTitle .
  OPTIONAL { ?book dbp:importance ?importance } .
  OPTIONAL { ?book dbp:popularity ?popularity } .
}
ORDER BY ?bookTitle
---

Explanation:

The first two lines define the prefixes for the DBpedia ontology and property namespaces.
The SELECT statement retrieves the title, importance, and popularity of the books authored by Borges using the WHERE clause.
The WHERE clause specifies that the book must have Borges as the author using the dbo:author property, and must be a notable work using the dbo:notableWork property.
The ?book variable refers to the book URI, and ?bookTitle refers to the book title.
The OPTIONAL clauses retrieve the importance and popularity of the books, if available, using the dbp:importance and dbp:popularity properties.
The DISTINCT keyword ensures that there are no duplicate results.
The results are sorted in ascending order by book title using the ORDER BY clause.


Answer:

Here are some results from this query:

bookTitle	                                |                importance	              |            popularity
---------------------------------------------------------------------------------------------------------------
A Universal History of Iniquity		
Atlas		
Dreamtigers		
El aleph	                                   One of Borges's most famous short stories.	
El hacedor		  
Ficciones	                                   One of the most important works of 20th-century Latin American literature.	
Historia universal de la infamia		
Inquisiciones		
La cuestión del otro		
La muerte y la brújula		
La biblioteca de Babel	                          One of Borges's most famous short stories.	
La lotería en Babilonia y otros relatos		
Nueve ensayos dantescos		
Otras inquisiciones		
Pierre Menard, autor del Quijote		
The Book of Imaginary Beings		
The Garden of Forking Paths	                 One of Borges's most famous short stories.	
The Maker		













(4) Kata level-3 (20/100 points; bal. 72)

Title: "What the Pho?"

Description:

You just tried pho noodle soup for the first time and wondered where they have been your whole life... you notice that not all pho noodle soups, however, are created equal. You now want to use your knowledge of SPARQL to appease the discriminating discernment of your palate, that is, to use your SPARQL knowledge to find the best Pho in town. Rather than search for reviews one platform at a time (e.g., yelp vs. grubhub vs. other) you have the brilliant idea to try and unify your search with SPARQL. Write a SPARQL query to find the best Pho restaurants in Buffalo, NY based on ratings from multiple platforms. The query should take into consideration the ratings of each restaurant on Grubhub.com, UberEats.com, DoorDash.com, and Yelp.com.

The query should return the name, address, and average rating of each restaurant. The restaurants should be ordered in descending order based on their average rating.

Tags: SPARQL, restaurants, Pho, Buffalo, ratings, Grubhub, UberEats, DoorDash, Yelp

Example:

Consider the following RDF data (Turtle):

@prefix schema: <http://schema.org/> .
@prefix ex: <http://example.org/> .

ex:restaurant1 a schema:Restaurant ;
    schema:name "Pho 99" ;
    schema:address "357 Connecticut St, Buffalo, NY 14213" ;
    schema:aggregateRating [
        a schema:AggregateRating ;
        schema:ratingValue "4.2" ;
        schema:reviewCount "75" ;
        schema:reviewRating [
            a schema:Rating ;
            schema:author "Grubhub" ;
            schema:ratingValue "4.2"
        ]
    ].

ex:restaurant2 a schema:Restaurant ;
    schema:name "Pho Dollar" ;
    schema:address "322 W Ferry St, Buffalo, NY 14213" ;
    schema:aggregateRating [
        a schema:AggregateRating ;
        schema:ratingValue "4.5" ;
        schema:reviewCount "45" ;
        schema:reviewRating [
            a schema:Rating ;
            schema:author "UberEats" ;
            schema:ratingValue "4.5"
        ]
    ].

ex:restaurant3 a schema:Restaurant ;
    schema:name "Pho 54" ;
    schema:address "128 Elmwood Ave, Buffalo, NY 14201" ;
    schema:aggregateRating [
        a schema:AggregateRating ;
        schema:ratingValue "4.8" ;
        schema:reviewCount "65" ;
        schema:reviewRating [
            a schema:Rating ;
            schema:author "DoorDash" ;
            schema:ratingValue "4.8"
        ]
    ].

ex:restaurant4 a schema:Restaurant ;
    schema:name "Pho Lantern" ;
    schema:address "837 Niagara St, Buffalo, NY 14213" ;
    schema:aggregateRating [
        a schema:AggregateRating ;
        schema:ratingValue "4.7" ;
        schema:reviewCount "90" ;
        schema:reviewRating [
            a schema:Rating ;
            schema:author "Yelp" ;
            schema:ratingValue "4.7"
        ]
    ].

The following query should return the best Pho restaurants in Buffalo, NY based on ratings from multiple platforms (SPARQL):

---
PREFIX schema: <http://schema.org/>
SELECT ?name ?address (AVG(?ratingValue) AS ?averageRating)
WHERE {
    ?restaurant a schema:Restaurant ;
                schema:name ?name ;
                schema:address ?address ;
                schema:aggregateRating ?aggregateRating .

    ?aggregateRating schema:ratingValue ?ratingValue ;
                      schema:reviewRating ?reviewRating .

    ?reviewRating schema:author ?author .

    FILTER (REGEX(?address, "Buffalo, NY") && (?author = "Grubhub" || ?author = "UberEats" || ?author = "DoorDash" || ?author = "Yelp"))
}
GROUP BY ?name ?address
ORDER BY DESC(?averageRating)
---

Output:

---
Pho 99
Pho Cali
Pho Lantern
---

Tags:
SPARQL
RDF
Restaurants
Ratings
Pho
Buffalo
New York


Test Cases:
The following test cases will test your SPARQL query on different RDF datasets.

Test Case 1
Input RDF dataset:

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix schema: <http://schema.org/> .
@prefix ubereats: <http://schema.org/OrderAction/ubereats/> .
@prefix doordash: <http://schema.org/OrderAction/doordash/> .
@prefix grubhub: <http://schema.org/OrderAction/grubhub/> .
@prefix yelp: <http://schema.org/Review/yelp/> .

<http://example.com/restaurant/1> a





































(5) Kata level-3 (20/100 points; bal. 52)

Title: RDF Data Analysis and Visualization of scientific publications...

Description:

You are given a large RDF dataset that contains information about scientific publications, their authors, and their citations. Your task is to write a SPARQL query that extracts and visualizes the most influential authors and their citation networks.

The query should:

Identify the top 20 authors with the highest number of publications.
For each author, retrieve their publications and the authors they have cited.
For each cited author, retrieve their publications and the authors they have cited.
Repeat step 3 until a maximum depth of 3 is reached.
Visualize the resulting citation network using a graph visualization tool of your choice.
The query will be evaluated based on its efficiency, correctness, and the quality of the resulting visualization.

Sample Dataset:

A sample RDF dataset can be downloaded from the following URL: https://example.com/publications.rdf

Sample Output:

The output of the query should be a graph visualization that shows the citation network of the top 20 authors. The nodes in the graph should represent authors, and the edges should represent citation links between publications.

Test Cases:

The query should be tested on a large RDF dataset that contains a variety of scientific publications, authors, and citation links. The dataset should also contain some invalid or incomplete data to ensure that the query can handle such scenarios.

Here are some example test cases:

The query should return the correct number of authors and their publications.
The query should correctly identify the citation links between publications and authors.
The visualization should accurately represent the citation network and allow the user to interact with the graph to explore the data.
The query should be efficient and able to handle large datasets in a reasonable amount of time.

The following is a general outline of the steps that could be taken to solve this kata:

Identify the top 20 authors with the highest number of publications using the COUNT() and ORDER BY functions in SPARQL.
Retrieve the publications and authors cited by each top author using the SELECT, WHERE, and FILTER clauses in SPARQL.
Iterate over the cited authors and retrieve their publications and authors cited until a maximum depth of 3 is reached. This can be achieved using SPARQL subqueries or recursive queries.
Transform the resulting data into a graph format that can be visualized using a graph visualization tool such as D3.js or NetworkX.
Visualize the graph and allow the user to interact with it to explore the citation network.
The query will need to be tested and refined to ensure that it correctly handles invalid or incomplete data and can efficiently handle large datasets.

Proposed answer to SPARQL query:

---
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

# Identify the top 20 authors with the highest number of publications
SELECT ?author (COUNT(?pub) AS ?numPublications)
WHERE {
  ?pub rdf:type ?type .
  ?type rdfs:subClassOf* <http://purl.org/spar/fabio/Expression> .
  ?pub <http://purl.org/dc/elements/1.1/creator> ?author .
}
GROUP BY ?author
ORDER BY DESC(?numPublications)
LIMIT 20

# Retrieve the publications and authors cited by each top author
SELECT ?author ?pub ?citedAuthor
WHERE {
  ?pub rdf:type ?type .
  ?type rdfs:subClassOf* <http://purl.org/spar/fabio/Expression> .
  ?pub <http://purl.org/dc/elements/1.1/creator> ?author .
  ?pub <http://purl.org/spar/fabio/cites> ?cited .
  ?cited <http://purl.org/dc/elements/1.1/creator> ?citedAuthor .
}
ORDER BY ?author

# Retrieve the publications and authors cited by each cited author
SELECT ?citedAuthor ?pub ?citedAuthor2
WHERE {
  ?pub rdf:type ?type .
  ?type rdfs:subClassOf* <http://purl.org/spar/fabio/Expression> .
  ?pub <http://purl.org/dc/elements/1.1/creator> ?citedAuthor .
  ?pub <http://purl.org/spar/fabio/cites> ?cited .
  ?cited <http://purl.org/dc/elements/1.1/creator> ?citedAuthor2 .
}
ORDER BY ?citedAuthor
---

This query includes three parts that address the requirements of the kata:

Identify the top 20 authors with the highest number of publications using the COUNT() and ORDER BY functions in SPARQL.
Retrieve the publications and authors cited by each top author using the SELECT, WHERE, and FILTER clauses in SPARQL.
Retrieve the publications and authors cited by each cited author using the SELECT, WHERE, and FILTER clauses in SPARQL.
The resulting data can then be transformed into a graph format that can be visualized using a graph visualization tool such as D3.js or NetworkX.























(6) Kata level-4 (10/100 points; bal. 42)

Title: Tracing the Path of Scientific Knowledge 

Description: You have been given a SPARQL endpoint to a Knowledge Graph that contains information about scientists and their scientific discoveries. The graph includes the scientist's name, birthplace, birthdate, disciplines they worked in, and discoveries for which they are known. The KG is reproduced below.

—
Knowledge Graph (KG) for Scientists and their Discoveries

The KG represents information about important scientists and their scientific discoveries. It includes details such as the scientist's name, birthplace, birthdate, disciplines they worked in, and their notable discoveries.

Example KG Triples:
- - - 
# Scientist: Albert Einstein
ex:scientist1 rdf:type ex:Scientist ;
              ex:name "Albert Einstein" ;
              ex:birthplace "Ulm, Germany" ;
              ex:birthdate "14 March 1879" ;
              ex:discipline ex:Physics ;
              ex:discovery ex:einstein_discovery .

ex:einstein_discovery rdf:type ex:Discovery ;
                      ex:name "Theory of Relativity" ;
                      ex:date "1905" .

# Scientist: Marie Curie
ex:scientist2 rdf:type ex:Scientist ;
              ex:name "Marie Curie" ;
              ex:birthplace "Warsaw, Poland" ;
              ex:birthdate "7 November 1867" ;
              ex:discipline ex:Physics ;
              ex:discovery ex:curie_discovery .

ex:curie_discovery rdf:type ex:Discovery ;
                   ex:name "Radioactivity" ;
                   ex:date "1898" .

# Scientist: Charles Darwin
ex:scientist3 rdf:type ex:Scientist ;
              ex:name "Charles Darwin" ;
              ex:birthplace "Shrewsbury, England" ;
              ex:birthdate "12 February 1809" ;
              ex:discipline ex:Biology ;
              ex:discovery ex:darwin_discovery .

ex:darwin_discovery rdf:type ex:Discovery ;
                    ex:name "Theory of Evolution" ;
                    ex:date "1859" .

# Disciplines
ex:Physics rdf:type ex:Discipline ;
           ex:name "Physics" .

ex:Biology rdf:type ex:Discipline ;
           ex:name "Biology" .

- - -

In this KG, the scientists (e.g., Albert Einstein, Marie Curie, Charles Darwin) are represented as instances of the class ex:Scientist. Each scientist has properties such as ex:name (name of the scientist), ex:birthplace (place of birth), ex:birthdate (date of birth), ex:discipline (the discipline they worked in), and ex:discovery (the notable discovery associated with the scientist).

The discoveries (e.g., Theory of Relativity, Radioactivity, Theory of Evolution) are represented as instances of the class ex:Discovery. Each discovery has properties such as ex:name (name of the discovery) and ex:date (date of the discovery).

The disciplines (e.g., Physics, Biology) are represented as instances of the class ex:Discipline. Each discipline has a property ex:name (name of the discipline).

This KG provides a foundation for querying and exploring information about scientists, their birthplaces, birthdates, disciplines, and discoveries. It can be used to answer questions about scientific contributions, compare scientists from different disciplines, and analyze relationships between scientists and their discoveries.

—

Task: Your task is to create a SPARQL query that traces the influence of one scientist's discovery on the work of others. Specifically, you should return a list of scientists who have made a discovery in the same discipline after the date of the original discovery. The list should be ordered by date of the discovery, and each entry should include the scientist's name, discovery, and date of the discovery.

Example Input/Output:

Ex. Input:

Original scientist: "Isaac Newton"
Original discovery: "Laws of Motion"
Discovery date: 1687

Ex. Output:

1. Scientist: "Leonhard Euler", Discovery: "Euler's Laws of Motion", Date: 1750
2. Scientist: "Henri Poincaré", Discovery: "Three-body Problem", Date: 1890
3. Scientist: "Albert Einstein", Discovery: "Theory of Relativity", Date: 1915

---

Test Cases:

The learner should provide a series of test cases with different scientists, disciplines, and dates. 

---

Solution:

```sparql
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX dbo: <http://dbpedia.org/ontology/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT ?scientist ?discovery ?date
WHERE {
    ?originalScientist foaf:name "Isaac Newton" .
    ?originalScientist dbo:discovery ?originalDiscovery .
    ?originalDiscovery dbo:name "Laws of Motion" ;
                      dbo:date ?originalDate .

    ?scientist dbo:discovery ?discovery .
    ?discovery dbo:discipline ?discipline ;
               dbo:date ?date .
    ?originalDiscovery dbo:discipline ?discipline .

    FILTER (?date > ?originalDate)
}
ORDER BY ?date
```

This query first finds the original scientist, discovery, and date. It then finds all other discoveries in the same discipline and filters out any that occurred before the original date. The results are ordered by the date of the discovery. The difficulty of this kata lies in understanding how to connect and filter the different types of information in the Knowledge Graph. The learner needs to be comfortable with SPARQL prefixes, filters, and ordering, as well as reasoning about how to structure the query to correctly trace the influence of a discovery through time.



















(7) Kata level-6 (3/100 points; bal. 39)

Title: Keanu Reeves is Everything (Or, “Keanu’s Movie Success Query”)

Description:
 
You just watched the four John Wick movies back-to-back. You’ve now learned that puppies and dogs in general are more important than human beings, and that John Wick is immune to car hits or falling from buildings. More importantly, you cannot get enough Keanu Reeves in your life. So you do what any reasonable SPARQL learner would do and try to come up with a query to find Mr. Reeves anywhere he could be. Of course, not all Keanu movies are created equal, so you want to have an idea of the ones worth watching first. Once you are done with the better ones, you’ll see how long you can go before getting Keanu withdrawals (it’s a real thing, look it up) and must descend all the way into B-level movies. Given a (hypothetical) SPARQL endpoint for IMDb and Rotten Tomatoes, write a query to fetch all movies that Keanu Reeves starred in, and organize them by their relative financial success at the box office and popularity.

```sparql
PREFIX imdb: <http://imdb.com/resource/>
PREFIX rt: <http://rottentomatoes.com/resource/>
```

Solution:

```sparql
PREFIX imdb: <http://imdb.com/resource/>
PREFIX rt: <http://rottentomatoes.com/resource/>

SELECT ?movie ?boxOffice ?popularity WHERE {
  ?movie imdb:starring imdb:keanu_reeves ;
         imdb:boxOffice ?boxOffice ;
         rt:popularity ?popularity .
} ORDER BY DESC(?boxOffice) DESC(?popularity)






Explanation:

The PREFIX directive is used to define the namespaces or prefixes for the given URIs.
The prefix imdb is associated with the URI http://imdb.com/resource/.
The prefix rt is associated with the URI http://rottentomatoes.com/resource/.
The SELECT clause specifies the variables to be retrieved in the query results.
The WHERE clause defines the pattern to match the desired data.
The ?movie variable represents the movies starring Keanu Reeves.
The ?boxOffice variable retrieves the box office information of the movies.
The ?popularity variable retrieves the popularity information of the movies.
The ORDER BY clause is used to sort the results in descending order based on the box office and popularity values.

n.b., the query for this kata assumes the presence of appropriate RDF data with IMDb and Rotten Tomatoes namespaces. 


























(8) Kata level-4 (10/100 points; bal. 29)

Title: Keanu Reeves' Movie Success SPARQL Analysis

Description:
 
This is a more difficult version of the previous kata. Given a (hypothetical) SPARQL endpoint for IMDb and Rotten Tomatoes, write a query to fetch all movies that Keanu Reeves starred in, and calculate their average box office earnings and average popularity. Additionally, retrieve the top 3 movies with the highest box office earnings and the top 3 movies with the highest popularity.

Solution:

PREFIX imdb: <http://imdb.com/resource/>
PREFIX rt: <http://rottentomatoes.com/resource/>

SELECT ?movie ?boxOffice ?popularity WHERE {
  {
    SELECT ?movie ?boxOffice ?popularity WHERE {
      ?movie imdb:starring imdb:Keanu_Reeves ;
             imdb:boxOffice ?boxOffice ;
             rt:popularity ?popularity .
    }
    ORDER BY DESC(?boxOffice) LIMIT 3
  }
  UNION
  {
    SELECT ?movie ?boxOffice ?popularity WHERE {
      ?movie imdb:starring imdb:Keanu_Reeves ;
             imdb:boxOffice ?boxOffice .
    }
    ORDER BY DESC(?boxOffice) LIMIT 3
  }
  UNION
  {
    SELECT ?movie ?boxOffice ?popularity WHERE {
      ?movie imdb:starring imdb:Keanu_Reeves ;
             rt:popularity ?popularity .
    }
    ORDER BY DESC(?popularity) LIMIT 3
  }
}


n.b., This kata would test an individual's understanding of advanced SPARQL features, including the use of aggregation functions (like AVG), UNION, and LIMIT. 



































(9) Kata level-2 (25/100 points; bal. 04)

Title: Netflix Shows Popularity Analysis & RDF Generation; a SPARQL-Python kata

Description: 

Assume a hypothetical SPARQL endpoint for Netflix, write a query to fetch all shows along with their popularity ranking and total number of seasons. Then, use the CONSTRUCT function to create a new RDF graph where each show is linked to its popularity ranking and season count. The new RDF graph should use the vocabulary from the Schema.org ontology. Use Python as needed.

Solution:
PREFIX netflix: <http://netflix.com/resource/>
PREFIX schema: <http://schema.org/>

CONSTRUCT {
  ?show schema:aggregateRating ?popularity .
  ?show schema:numberOfSeasons ?seasonCount .
}
WHERE {
  ?show a netflix:Show ;
        netflix:popularity ?popularity ;
        netflix:seasonCount ?seasonCount .
}


Tests (with Python):

from rdflib import Graph
from rdflib.compare import graph_diff

def test_construct_query():
    query = """
    PREFIX netflix: <http://netflix.com/resource/>
    PREFIX schema: <http://schema.org/>
    
    CONSTRUCT {
        ?show schema:aggregateRating ?popularity .
        ?show schema:numberOfSeasons ?seasonCount .
    }
    WHERE {
        ?show a netflix:Show ;
              netflix:popularity ?popularity ;
              netflix:seasonCount ?seasonCount .
    }
    """

    result_graph = run_query(query)

    # Define the expected output as an RDF graph
    expected_output = Graph()
    # Add your expected triples to the graph

    # Compare the result graph and the expected output graph
    _, _, in_both, in_result, in_expected = graph_diff(result_graph, expected_output)

    # Assert that both graphs have the same content
    assert len(in_result) == 0 and len(in_expected) == 0, "Result and expected output graphs are not the same"

test_construct_query()


n.b., The expected output would need to be replaced with the correct RDF graph. The actual values would depend on the current state of the (hypothetical) Netflix database. (I’ve proposed a rating of level-2 kyu for this kata. It would test an learner's understanding of advanced SPARQL features, including the CONSTRUCT clause, which is used to create a new RDF graph based on the results of a query.) It would also test the learner’s ability to use SPARQL with Python.












(10) Kata level-4 (10/100 points; bal. - 01) 

Title: Popular Movies by Box Office Success (2010-2020); a Python-SPARQL kata…

Comment on difficulty level and primary language: This kata would be for beginners who have a basic understanding of SPARQL and are familiar with parsing and manipulating data structures. This kata, to be clear, is for Python training that integrates SPARQL. It requires using Python, so I’ve rated it as a level-4 kyu kata. The previous kata required integrating Python with SPARQL, primarily for SPARQL learners; in a way, this kata does more or less the same thing, but from the other direction: it requires Python users to integrate SPARQL. 

Description:

Write a SPARQL query that retrieves and organizes popular movies released between 2010 and 2020 based on their box office success. Then implement a function that takes the SPARQL query as input, executes it against the Wikidata Query Service (WDQS) endpoint, and returns a list of movie objects containing relevant information such as the movie name, publication date, and box office revenue.

sparql
```
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX p: <http://www.wikidata.org/prop/>
PREFIX ps: <http://www.wikidata.org/prop/statement/>
PREFIX pq: <http://www.wikidata.org/prop/qualifier/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX wikibase: <http://wikiba.se/ontology#>
PREFIX bd: <http://www.bigdata.com/rdf#>

SELECT ?movie ?movieLabel ?publicationDate ?boxOffice WHERE {
  ?movie wdt:P31 wd:Q11424; # Instance of film
         wdt:P577 ?publicationDate; # Publication date
         p:P2142/ps:P2142 ?boxOffice; # Box office
         p:P577/ps:P577 ?dateStatement. # Publication date statement

  # Filter by date range 2010-2020
  FILTER (YEAR(?publicationDate) >= 2010 && YEAR(?publicationDate) <= 2020)

  # Optional: Filter movies with at least $100,000,000 in box office
  FILTER (?boxOffice >= 100000000)

  # Optional: Use qualifiers to refine publication date
  OPTIONAL { ?dateStatement pq:P2919 ?granularity. }
  FILTER (!BOUND(?granularity) || ?granularity = wd:Q577) # Ensure granularity is a year or not present

  # Labels
  SERVICE wikibase:label { bd:serviceParam wikibase:language "en". ?movie rdfs:label ?movieLabel. }
} 
ORDER BY DESC(?boxOffice)
LIMIT 100
```


Function Signature:
```python
def retrieve_popular_movies(query: str) -> List[dict]:
    pass
```

### Example Usage:
```python
query = '''
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX p: <http://www.wikidata.org/prop/>
PREFIX ps: <http://www.wikidata.org/prop/statement/>
PREFIX pq: <http://www.wikidata.org/prop/qualifier/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX wikibase: <http://wikiba.se/ontology#>
PREFIX bd: <http://www.bigdata.com/rdf#>

SELECT ?movie ?movieLabel ?publicationDate ?boxOffice WHERE {
  ?movie wdt:P31 wd:Q11424; # Instance of film
         wdt:P577 ?publicationDate; # Publication date
         p:P2142/ps:P2142 ?boxOffice; # Box office
         p:P577/ps:P577 ?dateStatement. # Publication date statement

  # Filter by date range 2010-2020
  FILTER (YEAR(?publicationDate) >= 2010 && YEAR(?publicationDate) <= 2020)

  # Optional: Filter movies with at least $100,000,000 in box office
  FILTER (?boxOffice >= 100000000)

  # Optional: Use qualifiers to refine publication date
  OPTIONAL { ?dateStatement pq:P2919 ?granularity. }
  FILTER (!BOUND(?granularity) || ?granularity = wd:Q577) # Ensure granularity is a year or not present

  # Labels
  SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
} 
ORDER BY DESC(?boxOffice)
LIMIT 100
'''

movies = retrieve_popular_movies(query)
print(movies)
```

### Expected Output:
```python
[
    {
        'movie': 'http://www.wikidata.org/entity/Q12345',
        'movieLabel': 'Movie Name 1',
        'publicationDate': '2012-05-21T00:00:00Z',
        'boxOffice': '1234567890'
    },
    {
        'movie': 'http://www.wikidata.org/entity/Q23456',
        'movieLabel': 'Movie Name 2',
        'publicationDate': '2014-09-15T00:00:00Z',
        'boxOffice': '987654321'
    },
    # Additional movie objects...
]
```

### Kata Solution:

```python
from typing import List, Dict
from SPARQLWrapper import SPARQLWrapper, JSON

def retrieve_popular_movies(query: str) -> List[Dict]:
    sparql = SPARQLWrapper("https://query.wikidata.org/sparql")
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    
    results = sparql.query().convert()

    movies = []
    for result in results["results"]["bindings"]:
        movie = {
            'movie': result['movie']['value'],
            'movieLabel': result['movieLabel']['value'],
            'publicationDate': result['publicationDate']['value'],
            'boxOffice': result['boxOffice']['value']
        }
        movies.append(movie)

    return movies
```

***This function should work with the example usage above: 

```python
query = '''
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX p: <http://www.wikidata.org/prop/>
PREFIX ps: <http://www.wikidata.org/prop/statement/>
PREFIX pq: <http://www.wikidata.org/prop/qualifier/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX wikibase: <http://wikiba.se/ontology#>
PREFIX bd: <http://www.bigdata.com/rdf#>

SELECT ?movie ?movieLabel ?publicationDate ?boxOffice WHERE {
  ?movie wdt:P31 wd:Q11424; # Instance of film
         wdt:P577 ?publicationDate; # Publication date
         p:P2142/ps:P2142 ?boxOffice; # Box office
         p:P577/ps:P577 ?dateStatement. # Publication date statement

  # Filter by date range 2010-2020
  FILTER (YEAR(?publicationDate) >= 2010 && YEAR(?publicationDate) <= 2020)

  # Optional: Filter movies with at least $100,000,000 in box office
  FILTER (?boxOffice >= 100000000)

  # Optional: Use qualifiers to refine publication date
  OPTIONAL { ?dateStatement pq:P2919 ?granularity. }
  FILTER (!BOUND(?granularity) || ?granularity = wd:Q577) # Ensure granularity is a year or not present

  # Labels
  SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
}
ORDER BY DESC(?boxOffice)
LIMIT 100
'''

movies = retrieve_popular_movies(query)
print(movies)
```

n.b., The above should return a list of movie dictionaries containing movie data as specified in the expected output. You’ll need to install the SPARQLWrapper library first, with:

pip install SPARQLWrapper



















(11) Kata level-3 (20/100 points; bal. -21) 

Title: Linked Data “HIT” Challenge

Description:

In this advanced multi-step kata, you will tackle a series of complex SPARQL queries to solve challenging problems in the realm of Linked Data. You'll need to leverage your knowledge of SPARQL syntax, patterns, and operators to extract valuable information from interconnected datasets. Get ready for an High-Intensity-Training (HIT) SPARQL workout!

Instructions/taks for this multi-step kata:

1. Retrieve Information: Write a SPARQL query to retrieve the names and birth dates of all individuals in the dataset.

2. Filtering and Sorting: Modify your previous query to filter the results and retrieve only the individuals who were born after a specific year. Then sort the resulting individuals in descending order based on their birth dates.

3. Joins and Relationships: Write a SPARQL query to retrieve the names of individuals who have a specific relationship with a particular person, identified by their unique identifier. Then extend the previous query to include the names of the relationships between the individuals and the specific person.

4. Aggregation and Grouping: Write a SPARQL query to calculate the average age of individuals in the dataset. Group the individuals based on their nationalities and retrieve the total count of individuals per nationality.

5. Complex Queries: Write a SPARQL query to find the names of individuals who are connected to a specific person by a relationship chain of at most four individuals.

6. Optional: Write additional SPARQL queries to further explore the dataset and showcase your skills. Graphs welcome!


Solution, with step-by-step explanation(s):

1. Retrieve Information:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?birthDate WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:birthDate ?birthDate .
     }
     ```
 
Explanation: This query retrieves the names and birth dates of all individuals in the dataset. It uses the `rdf:type` property to specify that the individuals should be of type `foaf:Person`. The `foaf:name` and `ex:birthDate` properties are used to retrieve the respective values.


2. Filtering and Sorting:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?birthDate WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:birthDate ?birthDate .
       FILTER(YEAR(?birthDate) > 1990)
     }
     ORDER BY DESC(?birthDate)
     ```
Explanation: This query builds upon the previous query and adds a filter to retrieve only individuals who were born after a specific year (in this case, 1990). The `YEAR` function is used to extract the year from the `ex:birthDate` property. The `ORDER BY` clause is used to sort the results in descending order based on the birth dates.


3. Joins and Relationships:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?relationship WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:relationship ?relationship .
       ?individual ex:hasRelationshipWith ex:specificPerson .
     }
     ```

Explanation: This query retrieves the names of individuals who have a specific relationship with a particular person, identified by the unique identifier `ex:specificPerson`. The `ex:relationship` property is used to retrieve the names of the relationships between the individuals and the specific person.


4. Aggregation and Grouping:
   - SPARQL Query:
     ```sparql
     SELECT (AVG(?age) AS ?averageAge) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:age ?age .
     }
     ```
Explanation: This query calculates the average age of all individuals in the dataset. The `AVG` function is used to calculate the average value of the `ex:age` property.

     ```sparql
     SELECT ?nationality (COUNT(?individual) AS ?count) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:nationality ?nationality .
     }
     GROUP BY ?nationality
     ```

Explanation: This query groups the individuals based on their nationalities and retrieves the total count of individuals per nationality. The `COUNT` function is used to count the number of occurrences of each nationality.


5. Complex Queries:
   - SPARQL Query:
     ```sparql
     SELECT ?name WHERE {
       ex:specificPerson ex:hasRelationshipWith ?person1 .
       OPTIONAL {
         ?person1 ex:hasRelationshipWith ?person2 .
         OPTIONAL {
           ?person2 ex:hasRelationshipWith ?person3 .
           OPTIONAL {
             ?person3 ex:hasRelationshipWith ?person4 .
             ?person4 rdf:type foaf:Person .
             ?person4 foaf:name ?name .
           }
           FILTER(?person3 != ex:specificPerson && ?person4 != ex:specificPerson)
         }
         FILTER(?person2 != ex:specificPerson && ?person3 !=

Explanation: This query finds the names of individuals who are connected to a specific person by a relationship chain of at most three individuals. It uses the `ex:specificPerson` as the starting point and then uses the `ex:hasRelationshipWith` property to traverse the relationships. The query uses optional patterns to allow for relationships of different lengths. The `FILTER` clauses ensure that the intermediate individuals in the chain and the final individual are not the same as the specific person.

6. Optional: Explore the dataset further and write additional SPARQL queries based on your interests or specific requirements…


Answers:

1. Retrieve Information:
   - Query:
     ```sparql
     SELECT ?name WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
     }
     ```
   - Example Result:
     ```
     +-------------------+
     |       name        |
     +-------------------+
     |   John Dog       |
     |   Jane Dog       |
     |   David Puppy  |
     |   Emily Kitty     |
     +-------------------+
     ```

2. Filtering and Sorting:
   - Query:
     ```sparql
     SELECT ?name WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:age ?age .
       FILTER(?age > 30)
     }
     ORDER BY ?birthDate
     ```
   - Example Result:
     ```
     +-------------------+
     |       name        |
     +-------------------+
     |   Jane Dog      |
     |   David Puppy |
     +-------------------+
     ```

3. Joins and Relationships:
   - Query:
     ```sparql
     SELECT ?name ?relationship WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:relationship ?relationship .
       ?individual ex:hasRelationshipWith ex:specificPerson .
     }
     ```
   - Example Result:
     ```
     +-------------------+----------------------+
     |       name        |    relationship      |
     +-------------------+----------------------+
     |   Jane dog      |   Friend             |
     |   David puppy |   Colleague          |
     +-------------------+----------------------+
     ```

4. Aggregation and Grouping:
   - Query:
     ```sparql
     SELECT (AVG(?age) AS ?averageAge) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:age ?age .
     }
     ```
   - Example Result:
     ```
     +-------------------+
     |    averageAge     |
     +-------------------+
     |   42.5            |
     +-------------------+
     ```

     ```sparql
     SELECT ?nationality (COUNT(?individual) AS ?count) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:nationality ?nationality .
     }
     GROUP BY ?nationality
     ```
   - Example Result:
     ```
     +-------------------+-------+
     |    nationality    | count |
     +-------------------+-------+
     |   United States   |   2   |
     |   United Kingdom  |   1   |
     |   Canada          |   1   |
     +-------------------+-------+
     ```

5. Complex Queries:
   - Query:
     ```sparql
     SELECT ?name WHERE {
       ex:specificPerson ex:hasRelationshipWith ?person1 .
       OPTIONAL {
         ?person1 ex:hasRelationshipWith ?person2 .
         OPTIONAL {
           ?person2 ex:hasRelationshipWith ?person3 .
           ?person3 rdf:type foaf


***Concrete Application/Use Case: 

Here's an example concrete use case for this kata…

Analyzing Population Distribution

Description: The dataset contains information about individuals, including their names, birth dates, nationalities, and locations. The goal is to analyze the population distribution based on nationality and age.

Solution:

1. Retrieve Information:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?birthDate WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:birthDate ?birthDate .
     }
     ```
Explanation: This query retrieves the names and birth dates of all individuals in the dataset. It provides a starting point for further analysis.


2. Filtering and Sorting:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?birthDate WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:birthDate ?birthDate .
       FILTER(YEAR(?birthDate) > 1990)
     }
     ORDER BY DESC(?birthDate)
     ```
Explanation: This query filters the individuals based on their birth dates, retrieving only those born after 1990. By sorting the results in descending order of birth dates, we can identify the youngest individuals in the dataset.


3. Joins and Relationships:
   - SPARQL Query:
     ```sparql
     SELECT ?name ?relationship WHERE {
       ?individual rdf:type foaf:Person .
       ?individual foaf:name ?name .
       ?individual ex:relationship ?relationship .
       ?individual ex:hasRelationshipWith ex:specificPerson .
     }
     ```
   - Explanation: This query retrieves the names of individuals who have a specific relationship with a particular person, identified by the unique identifier `ex:specificPerson`. By examining the relationships, we can identify connections and associations within the dataset.


4. Aggregation and Grouping:
   - SPARQL Query:
     ```sparql
     SELECT (AVG(?age) AS ?averageAge) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:age ?age .
     }
     ```
Explanation: This query calculates the average age of all individuals in the dataset. By aggregating the ages, we can gain insights into the overall age distribution.


     ```sparql
     SELECT ?nationality (COUNT(?individual) AS ?count) WHERE {
       ?individual rdf:type foaf:Person .
       ?individual ex:nationality ?nationality .
     }
     GROUP BY ?nationality
     ```
Explanation: This query groups the individuals based on their nationalities and retrieves the total count of individuals per nationality. It provides information on the distribution of individuals across different nationalities.


5. Complex Queries:
   - SPARQL Query:
     ```sparql
     SELECT ?name WHERE {
       ex:specificPerson ex:hasRelationshipWith ?person1 .
       OPTIONAL {
         ?person1 ex:hasRelationshipWith ?person2 .
         OPTIONAL {
           ?person2 ex:hasRelationshipWith ?person3 .
           OPTIONAL {
             ?person3 ex:hasRelationshipWith ?person4 .
             ?person4 rdf:type foaf:Person .
             ?person4 foaf:name ?name .
           }
           FILTER(?person3 != ex:specificPerson && ?person4 != ex:specificPerson)
         }
         FILTER(?person2 != ex:specificPerson && ?person3 != ex:specificPerson)
       }
       FILTER(?person1 != ex:specificPerson)
     }
     ```
Explanation: This query finds the names of individuals who are connected to a specific person by a relationship chain of at most four individuals. It explores the relationships within the dataset, allowing for multiple levels of connections.

6. Optional: Explore the dataset further and write additional SPARQL queries…


***Here's an additional example of a concrete use case for this Data Manipulation Challenge within an applied ontology context.

Applied Ontology Use Case: Managing Employee Data in an Organization

Description: You have an ontology representing an organization's employee data. The ontology contains information such as employee names, ages, nationalities, and positions. You need to manipulate the employee data using SPARQL update queries to reflect changes and updates within the organization.

Solution:

1. Insert Data:
   - SPARQL Query:
     ```sparql
     INSERT DATA {
       ex:employee1 rdf:type ex:Employee ;
                     foaf:name "John Doe" ;
                     ex:age 30 ;
                     ex:nationality "United States" ;
                     ex:position "Manager" .
     }
     ```
 
Explanation: This query inserts a new employee into the dataset. It uses the `INSERT DATA` statement to specify the employee's data. The new employee is identified by the unique identifier `ex:employee1` and has properties such as `foaf:name`, `ex:age`, `ex:nationality`, and `ex:position`.

2. Update Data:
   - SPARQL Query:
     ```sparql
     DELETE {
       ex:employee2 ex:age ?oldAge .
     }
     INSERT {
       ex:employee2 ex:age 35 .
     }
     WHERE {
       ex:employee2 rdf:type ex:Employee ;
                    ex:age ?oldAge .
     }
     ```

Explanation: This query updates the age of an existing employee identified by `ex:employee2`. It uses the `DELETE` and `INSERT` clauses to perform the update operation. The `WHERE` clause identifies the employee and retrieves their current age using the `ex:age` property. The `DELETE` clause removes the old age value, and the `INSERT` clause adds the new age value.


3. Delete Data:
   - SPARQL Query:
     ```sparql
     DELETE {
       ?employee ?property ?value .
     }
     WHERE {
       ?employee rdf:type ex:Employee ;
                 ?property ?value .
       FILTER(?employee = ex:employee3)
     }
     ```

Explanation: This query deletes an employee and all their associated information from the dataset. It uses the `DELETE` clause to specify the data to be deleted. The `WHERE` clause identifies the employee using `ex:employee3` and retrieves all properties and values associated with that employee. The `FILTER` clause ensures that only the specified employee is affected by the deletion.


4. Bulk Insert:
   - SPARQL Query:
     ```sparql
     INSERT DATA {
       ex:employee4 rdf:type ex:Employee ;
                     foaf:name "Alice" ;
                     ex:age 25 ;
                     ex:nationality "Canada" ;
                     ex:position "Analyst" .
       ex:employee5 rdf:type ex:Employee ;
                     foaf:name "Bob" ;
                     ex:age 35 ;
                     ex:nationality "United Kingdom" ;
                     ex:position "Engineer" .
       ex:employee6 rdf:type ex:Employee ;
                     foaf:name "Carol" ;
                     ex:age 40 ;
                     ex:nationality "Germany" ;
                     ex:position "Manager" .
     }
     ```

Explanation: This query performs a bulk insert operation to add multiple employees into the dataset. Each employee is identified by a unique identifier (`ex:employee4`, `ex:employee5`, `ex:employee6`) and has properties such as `foaf:name`, `ex:age`, `ex:nationality`, and `ex:position`. 



+++

***Endnote: The above should add up to 121 points; even on a more conservative rating of the katas, it should add up to at least 100 points. I am happy to provide more katas if this were not the case.






















